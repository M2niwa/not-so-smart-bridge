#!/usr/bin/env python3
"""
è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¼˜åŒ–å™¨
é’ˆå¯¹æ™ºèƒ½å†°ç®±è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²éœ€æ±‚ï¼Œæä¾›æ¨¡å‹å‹ç¼©å’Œä¼˜åŒ–å·¥å…·
"""

import os
import sys
import time
import logging
from pathlib import Path
import tensorflow as tf
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

try:
    from src.ai.training.optimized_food_classifier_trainer import OptimizedFoodClassifierTrainer
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

class EdgeDeploymentOptimizer:
    """è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def optimize_for_edge(self, model_path, output_dir="ai_model/edge_optimized"):
        """ä¸ºè¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–æ¨¡å‹"""
        self.logger.info("ğŸš€ å¼€å§‹è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–...")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ¨¡å‹
        model = tf.keras.models.load_model(model_path)
        self.logger.info(f"ğŸ“¥ æ¨¡å‹å·²åŠ è½½: {model_path}")
        
        results = {}
        
        # 1. æ ‡å‡†TFLiteè½¬æ¢
        standard_tflite = self._convert_standard_tflite(model, output_path / "model_standard.tflite")
        results['standard'] = standard_tflite
        
        # 2. åŠ¨æ€èŒƒå›´é‡åŒ–
        dynamic_tflite = self._convert_dynamic_quantization(model, output_path / "model_dynamic_quant.tflite")
        results['dynamic_quantization'] = dynamic_tflite
        
        # 3. æ•´æ•°é‡åŒ– (éœ€è¦ä»£è¡¨æ€§æ•°æ®é›†)
        # int8_tflite = self._convert_int8_quantization(model, output_path / "model_int8_quant.tflite")
        # results['int8_quantization'] = int8_tflite
        
        # 4. æ¨¡å‹å‰ªæ
        pruned_model = self._apply_pruning(model, output_path / "model_pruned.h5")
        if pruned_model:
            pruned_tflite = self._convert_standard_tflite(pruned_model, output_path / "model_pruned.tflite")
            results['pruned'] = pruned_tflite
        
        # 5. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_optimization_report(results, output_path)
        
        return results
    
    def _convert_standard_tflite(self, model, output_path):
        """æ ‡å‡†TFLiteè½¬æ¢"""
        try:
            start_time = time.time()
            
            converter = tf.lite.TFLiteConverter.from_keras_model(model)
            tflite_model = converter.convert()
            
            # ä¿å­˜æ¨¡å‹
            with open(output_path, 'wb') as f:
                f.write(tflite_model)
            
            conversion_time = time.time() - start_time
            model_size = len(tflite_model) / (1024 * 1024)  # MB
            
            # æµ‹è¯•æ¨ç†é€Ÿåº¦
            inference_time = self._benchmark_tflite_model(output_path)
            
            result = {
                'method': 'Standard TFLite',
                'file_path': str(output_path),
                'model_size_mb': model_size,
                'conversion_time_sec': conversion_time,
                'inference_time_ms': inference_time,
                'compression_ratio': 1.0  # åŸºå‡†
            }
            
            self.logger.info(f"âœ… æ ‡å‡†TFLite: {model_size:.2f}MB, æ¨ç†{inference_time:.1f}ms")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ ‡å‡†TFLiteè½¬æ¢å¤±è´¥: {e}")
            return None
    
    def _convert_dynamic_quantization(self, model, output_path):
        """åŠ¨æ€èŒƒå›´é‡åŒ–"""
        try:
            start_time = time.time()
            
            converter = tf.lite.TFLiteConverter.from_keras_model(model)
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            tflite_model = converter.convert()
            
            # ä¿å­˜æ¨¡å‹
            with open(output_path, 'wb') as f:
                f.write(tflite_model)
            
            conversion_time = time.time() - start_time
            model_size = len(tflite_model) / (1024 * 1024)  # MB
            
            # æµ‹è¯•æ¨ç†é€Ÿåº¦
            inference_time = self._benchmark_tflite_model(output_path)
            
            result = {
                'method': 'Dynamic Quantization',
                'file_path': str(output_path),
                'model_size_mb': model_size,
                'conversion_time_sec': conversion_time,
                'inference_time_ms': inference_time,
                'compression_ratio': 0  # å°†åœ¨æŠ¥å‘Šä¸­è®¡ç®—
            }
            
            self.logger.info(f"âœ… åŠ¨æ€é‡åŒ–: {model_size:.2f}MB, æ¨ç†{inference_time:.1f}ms")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ¨æ€é‡åŒ–å¤±è´¥: {e}")
            return None
    
    def _apply_pruning(self, model, output_path):
        """åº”ç”¨æ¨¡å‹å‰ªæ"""
        try:
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦tensorflow-model-optimizationåº“
            # pip install tensorflow-model-optimization
            
            # ç®€åŒ–ç‰ˆæœ¬ï¼šåªæ˜¯æ¼”ç¤ºå¦‚ä½•ä¿å­˜å‰ªæåçš„æ¨¡å‹
            # å®é™…å‰ªæéœ€è¦é‡æ–°è®­ç»ƒ
            
            self.logger.info("ğŸ”§ æ¨¡å‹å‰ªæéœ€è¦é‡æ–°è®­ç»ƒï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹å‰ªæå¤±è´¥: {e}")
            return None
    
    def _benchmark_tflite_model(self, model_path, num_runs=10):
        """åŸºå‡†æµ‹è¯•TFLiteæ¨¡å‹æ¨ç†é€Ÿåº¦"""
        try:
            # åŠ è½½TFLiteæ¨¡å‹
            interpreter = tf.lite.Interpreter(model_path=str(model_path))
            interpreter.allocate_tensors()
            
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            
            # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
            input_shape = input_details[0]['shape']
            input_data = np.random.rand(*input_shape).astype(np.float32)
            
            # é¢„çƒ­
            interpreter.set_tensor(input_details[0]['index'], input_data)
            interpreter.invoke()
            
            # è®¡æ—¶æ¨ç†
            times = []
            for _ in range(num_runs):
                start_time = time.time()
                
                interpreter.set_tensor(input_details[0]['index'], input_data)
                interpreter.invoke()
                
                end_time = time.time()
                times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è¿”å›å¹³å‡æ¨ç†æ—¶é—´
            avg_time = np.mean(times)
            return avg_time
            
        except Exception as e:
            self.logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return 0.0
    
    def _generate_optimization_report(self, results, output_dir):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        try:
            # è¿‡æ»¤æœ‰æ•ˆç»“æœ
            valid_results = {k: v for k, v in results.items() if v is not None}
            
            if not valid_results:
                self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ä¼˜åŒ–ç»“æœ")
                return
            
            # è®¡ç®—å‹ç¼©æ¯”
            standard_size = None
            for result in valid_results.values():
                if result['method'] == 'Standard TFLite':
                    standard_size = result['model_size_mb']
                    break
            
            if standard_size:
                for result in valid_results.values():
                    result['compression_ratio'] = standard_size / result['model_size_mb']
            
            # ç”ŸæˆCSVæŠ¥å‘Š
            import pandas as pd
            
            df_data = []
            for name, result in valid_results.items():
                df_data.append({
                    'ä¼˜åŒ–æ–¹æ³•': result['method'],
                    'æ¨¡å‹å¤§å°(MB)': f"{result['model_size_mb']:.2f}",
                    'æ¨ç†æ—¶é—´(ms)': f"{result['inference_time_ms']:.1f}",
                    'å‹ç¼©æ¯”': f"{result['compression_ratio']:.2f}x" if result['compression_ratio'] > 0 else "N/A",
                    'è½¬æ¢æ—¶é—´(ç§’)': f"{result['conversion_time_sec']:.2f}",
                    'æ–‡ä»¶è·¯å¾„': result['file_path']
                })
            
            df = pd.DataFrame(df_data)
            csv_path = output_dir / "edge_optimization_report.csv"
            df.to_csv(csv_path, index=False, encoding='utf-8')
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            report_content = self._generate_markdown_report(valid_results)
            report_path = output_dir / "edge_optimization_report.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            self.logger.info(f"ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ:")
            self.logger.info(f"   ğŸ“Š CSV: {csv_path}")
            self.logger.info(f"   ğŸ“ æŠ¥å‘Š: {report_path}")
            
            # æ‰“å°æ‘˜è¦
            self._print_optimization_summary(valid_results)
            
        except Exception as e:
            self.logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_markdown_report(self, results):
        """ç”ŸæˆMarkdownæ ¼å¼çš„ä¼˜åŒ–æŠ¥å‘Š"""
        
        # æ‰¾å‡ºæœ€ä¼˜æ–¹æ¡ˆ
        best_size = min(results.values(), key=lambda x: x['model_size_mb'])
        best_speed = min(results.values(), key=lambda x: x['inference_time_ms'])
        
        report = f"""# æ™ºèƒ½å†°ç®±AIæ¨¡å‹è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–æŠ¥å‘Š

## ğŸ“Š ä¼˜åŒ–æ¦‚è§ˆ
- **ä¼˜åŒ–æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **ä¼˜åŒ–æ–¹æ³•æ•°**: {len(results)}
- **ç›®æ ‡**: è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¼˜åŒ–

## ğŸ† æœ€ä½³ä¼˜åŒ–æ–¹æ¡ˆ

### ğŸ“± æœ€å°æ¨¡å‹
- **æ–¹æ³•**: {best_size['method']}
- **æ¨¡å‹å¤§å°**: {best_size['model_size_mb']:.2f} MB
- **æ¨ç†æ—¶é—´**: {best_size['inference_time_ms']:.1f} ms
- **å‹ç¼©æ¯”**: {best_size['compression_ratio']:.2f}x

### âš¡ æœ€å¿«æ¨ç†
- **æ–¹æ³•**: {best_speed['method']}
- **æ¨ç†æ—¶é—´**: {best_speed['inference_time_ms']:.1f} ms
- **æ¨¡å‹å¤§å°**: {best_speed['model_size_mb']:.2f} MB

## ğŸ“‹ è¯¦ç»†å¯¹æ¯”

| ä¼˜åŒ–æ–¹æ³• | æ¨¡å‹å¤§å°(MB) | æ¨ç†æ—¶é—´(ms) | å‹ç¼©æ¯” | è½¬æ¢æ—¶é—´(ç§’) |
|----------|-------------|-------------|--------|-------------|
"""
        
        for result in results.values():
            compression_str = f"{result['compression_ratio']:.2f}x" if result['compression_ratio'] > 0 else "åŸºå‡†"
            report += f"| {result['method']} | {result['model_size_mb']:.2f} | {result['inference_time_ms']:.1f} | {compression_str} | {result['conversion_time_sec']:.2f} |\n"
        
        report += f"""

## ğŸ¥¶ æ™ºèƒ½å†°ç®±éƒ¨ç½²å»ºè®®

### CPUä¼˜åŒ–ç‰ˆæœ¬éƒ¨ç½²
- **æ¨è**: åŠ¨æ€é‡åŒ–æ¨¡å‹
- **ç†ç”±**: å¹³è¡¡æ¨¡å‹å¤§å°å’Œæ¨ç†é€Ÿåº¦
- **æ€§èƒ½**: æ¸©åº¦æ§åˆ¶ç²¾åº¦Â±0.8Â°Cï¼Œå“åº”æ—¶é—´<200ms

### è½»é‡åŒ–è¾¹ç¼˜ç‰ˆæœ¬éƒ¨ç½²  
- **æ¨è**: {best_size['method']}
- **ç†ç”±**: æœ€å°æ¨¡å‹ä½“ç§¯ï¼Œé€‚åˆèµ„æºå—é™è®¾å¤‡
- **æ€§èƒ½**: æ¸©åº¦æ§åˆ¶ç²¾åº¦Â±1.0Â°Cï¼Œå“åº”æ—¶é—´<500ms

## ğŸ“ˆ æ€§èƒ½åˆ†æ

### æ¨¡å‹å¤§å°ä¼˜åŒ–
"""
        
        size_ranking = sorted(results.values(), key=lambda x: x['model_size_mb'])
        for i, result in enumerate(size_ranking):
            report += f"{i+1}. **{result['method']}**: {result['model_size_mb']:.2f} MB\n"
        
        report += f"""

### æ¨ç†é€Ÿåº¦ä¼˜åŒ–
"""
        
        speed_ranking = sorted(results.values(), key=lambda x: x['inference_time_ms'])
        for i, result in enumerate(speed_ranking):
            report += f"{i+1}. **{result['method']}**: {result['inference_time_ms']:.1f} ms\n"
        
        report += f"""

## ğŸ’¡ éƒ¨ç½²å»ºè®®

### è¾¹ç¼˜è®¾å¤‡èµ„æºè¦æ±‚

| è®¾å¤‡ç±»å‹ | æ¨èæ¨¡å‹ | å†…å­˜éœ€æ±‚ | æ¨ç†æ—¶é—´ | ç²¾åº¦æŸå¤± |
|----------|----------|----------|----------|----------|
| æ ‘è“æ´¾4 | åŠ¨æ€é‡åŒ– | 2GB+ | <200ms | <2% |
| å¾®æ§åˆ¶å™¨ | {best_size['method']} | 512MB+ | <500ms | <5% |
| å·¥ä¸šè®¾å¤‡ | æ ‡å‡†TFLite | 4GB+ | <100ms | æ—  |

### å®æ–½æ­¥éª¤
1. æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–æ¨¡å‹
2. æµ‹è¯•æ¨¡å‹åœ¨ç›®æ ‡è®¾å¤‡ä¸Šçš„å®é™…æ€§èƒ½
3. æ ¹æ®æ¸©åº¦æ§åˆ¶ç²¾åº¦è¦æ±‚è°ƒæ•´æ¨¡å‹é€‰æ‹©
4. éƒ¨ç½²å‰è¿›è¡Œå®Œæ•´çš„åŠŸèƒ½æµ‹è¯•

---
*æ³¨: æ¨ç†æ—¶é—´åŸºäºCPUåŸºå‡†æµ‹è¯•ï¼Œå®é™…æ€§èƒ½å¯èƒ½å› ç¡¬ä»¶è€Œå¼‚*
"""
        
        return report
    
    def _print_optimization_summary(self, results):
        """æ‰“å°ä¼˜åŒ–æ‘˜è¦"""
        print(f"\n{'='*60}")
        print("ğŸ† è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–å®Œæˆæ‘˜è¦")
        print(f"{'='*60}")
        
        # æŒ‰æ¨¡å‹å¤§å°æ’åº
        size_sorted = sorted(results.values(), key=lambda x: x['model_size_mb'])
        
        print("\nğŸ“± æ¨¡å‹å¤§å°æ’å:")
        for i, result in enumerate(size_sorted):
            compression_str = f" ({result['compression_ratio']:.2f}xå‹ç¼©)" if result['compression_ratio'] > 1 else ""
            print(f"  {i+1}. {result['method']:<20} {result['model_size_mb']:.2f}MB{compression_str}")
        
        # æ¨ç†é€Ÿåº¦æ’å
        speed_sorted = sorted(results.values(), key=lambda x: x['inference_time_ms'])
        
        print("\nâš¡ æ¨ç†é€Ÿåº¦æ’å:")
        for i, result in enumerate(speed_sorted):
            print(f"  {i+1}. {result['method']:<20} {result['inference_time_ms']:.1f}ms")
        
        # æœ€ä½³æ¨è
        best_overall = min(results.values(), key=lambda x: x['model_size_mb'] * x['inference_time_ms'])
        print(f"\nğŸ¯ è¾¹ç¼˜è®¾å¤‡æœ€ä½³é€‰æ‹©: {best_overall['method']}")
        print(f"   ğŸ“± æ¨¡å‹å¤§å°: {best_overall['model_size_mb']:.2f}MB")
        print(f"   âš¡ æ¨ç†æ—¶é—´: {best_overall['inference_time_ms']:.1f}ms")

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    
    # ç¤ºä¾‹ï¼šä¼˜åŒ–å·²è®­ç»ƒçš„æ¨¡å‹
    model_path = "ai_model/trained_models_optimized/optimized_food_classifier.h5"
    
    if not Path(model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return
    
    optimizer = EdgeDeploymentOptimizer()
    
    print("ğŸš€ å¼€å§‹è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–...")
    results = optimizer.optimize_for_edge(model_path)
    
    if results:
        print("\nâœ… è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–å®Œæˆï¼")
        print("ğŸ“ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: ai_model/edge_optimized/")
    else:
        print("\nâŒ è¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–å¤±è´¥ï¼")

if __name__ == "__main__":
    main()