#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„æ™ºèƒ½å†°ç®±AIæ¨¡å‹è®­ç»ƒè„šæœ¬
åŒ…å«æ•°æ®å¢å¼ºã€æ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ã€æ¨¡å‹è¯„ä¼°ç­‰ä¼˜åŒ–åŠŸèƒ½
"""

import os
import sys
import logging
import argparse
import json
import time
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from src.ai.training.data_preprocessor import DataPreprocessor
from src.ai.training.food_classifier_trainer import FoodClassifierTrainer
from src.ai.training.model_evaluator import ModelEvaluator

def setup_logging(log_level=logging.INFO, log_file=None):
    """è®¾ç½®ä¼˜åŒ–çš„æ—¥å¿—ç³»ç»Ÿ"""
    handlers = [logging.StreamHandler()]
    
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )

def check_data_quality(data_dir):
    """æ£€æŸ¥æ•°æ®é›†è´¨é‡å¹¶æä¾›å»ºè®®"""
    logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥æ•°æ®é›†ç»“æ„
    train_dir = Path(data_dir) / 'train'
    val_dir = Path(data_dir) / 'val'
    test_dir = Path(data_dir) / 'test'
    
    if not all([train_dir.exists(), val_dir.exists(), test_dir.exists()]):
        logger.warning("æ•°æ®é›†ç»“æ„ä¸å®Œæ•´ï¼Œç¼ºå°‘train/val/testç›®å½•")
        return False
    
    # ç»Ÿè®¡å„ç±»åˆ«å›¾åƒæ•°é‡
    categories = {}
    for split in ['train', 'val', 'test']:
        split_dir = Path(data_dir) / split
        categories[split] = {}
        
        for cat_dir in split_dir.iterdir():
            if cat_dir.is_dir():
                img_count = len(list(cat_dir.glob('*')))
                categories[split][cat_dir.name] = img_count
    
    # æ£€æŸ¥ç±»åˆ«ä¸€è‡´æ€§
    train_cats = set(categories['train'].keys())
    val_cats = set(categories['val'].keys())
    test_cats = set(categories['test'].keys())
    
    if not (train_cats == val_cats == test_cats):
        logger.warning("è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„ç±»åˆ«ä¸ä¸€è‡´")
        return False
    
    # è¾“å‡ºæ•°æ®é›†ç»Ÿè®¡
    logger.info("=== æ•°æ®é›†è´¨é‡æ£€æŸ¥ ===")
    total_images = 0
    for category in train_cats:
        train_count = categories['train'][category]
        val_count = categories['val'][category]
        test_count = categories['test'][category]
        cat_total = train_count + val_count + test_count
        total_images += cat_total
        
        logger.info(f"{category}: è®­ç»ƒ{train_count} + éªŒè¯{val_count} + æµ‹è¯•{test_count} = {cat_total}")
    
    logger.info(f"æ€»å›¾åƒæ•°: {total_images}")
    logger.info(f"ç±»åˆ«æ•°: {len(train_cats)}")
    
    # æ•°æ®å¹³è¡¡æ€§æ£€æŸ¥
    train_counts = list(categories['train'].values())
    if max(train_counts) / min(train_counts) > 3:
        logger.warning("æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ï¼Œå»ºè®®è¿›è¡Œç±»åˆ«å¹³è¡¡å¤„ç†")
    
    return True

def create_enhanced_data_generators(data_dir, batch_size=32, image_size=(224, 224)):
    """åˆ›å»ºå¢å¼ºçš„æ•°æ®ç”Ÿæˆå™¨"""
    logger = logging.getLogger(__name__)
    
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    
    # è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ï¼ˆå¼ºæ•°æ®å¢å¼ºï¼‰
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=30,          # å¢åŠ æ—‹è½¬è§’åº¦
        width_shift_range=0.3,      # å¢åŠ å¹³ç§»èŒƒå›´
        height_shift_range=0.3,
        shear_range=0.3,            # å¢åŠ å‰ªåˆ‡å˜æ¢
        zoom_range=0.3,             # å¢åŠ ç¼©æ”¾èŒƒå›´
        horizontal_flip=True,
        vertical_flip=False,        # é£Ÿç‰©å›¾åƒé€šå¸¸ä¸å‚ç›´ç¿»è½¬
        fill_mode='nearest',
        brightness_range=[0.8, 1.2], # äº®åº¦è°ƒæ•´
        channel_shift_range=20.0     # é¢œè‰²é€šé“åç§»
    )
    
    # éªŒè¯å’Œæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ï¼ˆä»…æ ‡å‡†åŒ–ï¼‰
    val_test_datagen = ImageDataGenerator(rescale=1./255)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    train_generator = train_datagen.flow_from_directory(
        str(Path(data_dir) / 'train'),
        target_size=image_size,
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=True,
        seed=42
    )
    
    val_generator = val_test_datagen.flow_from_directory(
        str(Path(data_dir) / 'val'),
        target_size=image_size,
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=False
    )
    
    test_generator = val_test_datagen.flow_from_directory(
        str(Path(data_dir) / 'test'),
        target_size=image_size,
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=False
    )
    
    logger.info(f"æ•°æ®ç”Ÿæˆå™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"  è®­ç»ƒé›†: {train_generator.samples} æ ·æœ¬")
    logger.info(f"  éªŒè¯é›†: {val_generator.samples} æ ·æœ¬")
    logger.info(f"  æµ‹è¯•é›†: {test_generator.samples} æ ·æœ¬")
    logger.info(f"  ç±»åˆ«æ•°: {len(train_generator.class_indices)}")
    
    return train_generator, val_generator, test_generator

def train_optimized_model(args):
    """ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒæµç¨‹"""
    logger = logging.getLogger(__name__)
    
    # 1. æ•°æ®è´¨é‡æ£€æŸ¥
    logger.info("=== æ­¥éª¤1: æ•°æ®è´¨é‡æ£€æŸ¥ ===")
    if not check_data_quality(args.data_dir):
        raise ValueError("æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥")
    
    # 2. åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
    logger.info("=== æ­¥éª¤2: åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨ ===")
    train_gen, val_gen, test_gen = create_enhanced_data_generators(
        args.data_dir, 
        batch_size=args.batch_size, 
        image_size=(args.image_size, args.image_size)
    )
    
    # 3. æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
    logger.info("=== æ­¥éª¤3: æ¨¡å‹æ„å»ºå’Œè®­ç»ƒ ===")
    num_classes = len(train_gen.class_indices)
    trainer = FoodClassifierTrainer(num_classes=num_classes, model_type=args.model_type)
    
    # æ„å»ºæ¨¡å‹
    model = trainer.build_model(input_shape=(args.image_size, args.image_size, 3))
    if model is None:
        raise ValueError("æ¨¡å‹æ„å»ºå¤±è´¥")
    
    logger.info(f"æ¨¡å‹æ¶æ„: {args.model_type}")
    logger.info(f"å‚æ•°æ•°é‡: {model.count_params():,}")
    
    # è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    train_success = trainer.train(
        train_generator=train_gen,
        validation_generator=val_gen,
        epochs=args.epochs,
        batch_size=args.batch_size
    )
    training_time = time.time() - start_time
    
    if not train_success:
        raise ValueError("æ¨¡å‹è®­ç»ƒå¤±è´¥")
    
    logger.info(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f} ç§’")
    
    # 4. å¾®è°ƒï¼ˆå¯é€‰ï¼‰
    if args.fine_tune:
        logger.info("=== æ­¥éª¤4: æ¨¡å‹å¾®è°ƒ ===")
        fine_tune_success = trainer.fine_tune(
            train_generator=train_gen,
            validation_generator=val_gen,
            epochs=args.fine_tune_epochs
        )
        
        if not fine_tune_success:
            logger.warning("æ¨¡å‹å¾®è°ƒå¤±è´¥")
    
    # 5. ä¿å­˜æ¨¡å‹
    logger.info("=== æ­¥éª¤5: ä¿å­˜æ¨¡å‹ ===")
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = output_dir / f"food_classifier_{args.model_type}_optimized.h5"
    trainer.save_model(str(model_path))
    
    # å¯¼å‡ºTFLiteæ¨¡å‹
    tflite_path = output_dir / f"food_classifier_{args.model_type}_optimized.tflite"
    trainer.export_tflite(str(model_path), str(tflite_path))
    
    # 6. ä¿å­˜ç±»åˆ«æ˜ å°„å’Œå…ƒæ•°æ®
    class_indices = train_gen.class_indices
    metadata = {
        'class_indices': class_indices,
        'num_classes': num_classes,
        'model_type': args.model_type,
        'image_size': args.image_size,
        'training_time': training_time,
        'total_params': int(model.count_params()),
        'dataset_info': {
            'train_samples': train_gen.samples,
            'val_samples': val_gen.samples,
            'test_samples': test_gen.samples
        }
    }
    
    with open(output_dir / 'model_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    # 7. ç»˜åˆ¶è®­ç»ƒå†å²
    if args.plot_history:
        logger.info("=== æ­¥éª¤6: ç»˜åˆ¶è®­ç»ƒå†å² ===")
        history_path = output_dir / f"training_history_{args.model_type}_optimized.png"
        trainer.plot_training_history(str(history_path))
    
    # 8. æ¨¡å‹è¯„ä¼°
    if args.evaluate:
        logger.info("=== æ­¥éª¤7: æ¨¡å‹è¯„ä¼° ===")
        evaluate_trained_model(trainer, test_gen, output_dir, class_indices)
    
    return model_path, class_indices, metadata

def evaluate_trained_model(trainer, test_generator, output_dir, class_indices):
    """è¯¦ç»†çš„æ¨¡å‹è¯„ä¼°"""
    logger = logging.getLogger(__name__)
    
    # åŸºæœ¬è¯„ä¼°
    metrics = trainer.evaluate(test_generator)
    
    # é¢„æµ‹æ‰€æœ‰æµ‹è¯•æ•°æ®
    test_generator.reset()
    predictions = trainer.model.predict(test_generator, verbose=1)
    y_pred = np.argmax(predictions, axis=1)
    y_true = test_generator.classes
    
    # åˆ†ç±»æŠ¥å‘Š
    class_names = list(class_indices.keys())
    report = classification_report(
        y_true, y_pred, 
        target_names=class_names, 
        output_dict=True
    )
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    eval_results = {
        'basic_metrics': metrics,
        'classification_report': report,
        'confusion_matrix': cm.tolist()
    }
    
    with open(output_dir / 'evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(eval_results, f, ensure_ascii=False, indent=2)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹ç±»åˆ«')
    plt.ylabel('çœŸå®ç±»åˆ«')
    plt.tight_layout()
    plt.savefig(output_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # è¾“å‡ºè¯„ä¼°æ‘˜è¦
    logger.info("=== æ¨¡å‹è¯„ä¼°ç»“æœ ===")
    logger.info(f"æµ‹è¯•å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    logger.info(f"æµ‹è¯•æŸå¤±: {metrics['loss']:.4f}")
    if 'top_k_accuracy' in metrics and metrics['top_k_accuracy']:
        logger.info(f"Top-Kå‡†ç¡®ç‡: {metrics['top_k_accuracy']:.4f}")
    
    logger.info("\nå„ç±»åˆ«æ€§èƒ½:")
    for class_name in class_names:
        if class_name in report:
            precision = report[class_name]['precision']
            recall = report[class_name]['recall']
            f1_score = report[class_name]['f1-score']
            logger.info(f"  {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1_score:.3f}")

def generate_training_summary(output_dir, metadata):
    """ç”Ÿæˆè®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
    logger = logging.getLogger(__name__)
    
    summary = f"""
# æ™ºèƒ½å†°ç®±AIæ¨¡å‹è®­ç»ƒæ€»ç»“æŠ¥å‘Š

## æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹ç±»å‹**: {metadata['model_type']}
- **å›¾åƒå°ºå¯¸**: {metadata['image_size']}x{metadata['image_size']}
- **ç±»åˆ«æ•°é‡**: {metadata['num_classes']}
- **å‚æ•°æ•°é‡**: {metadata['total_params']:,}

## æ•°æ®é›†ä¿¡æ¯
- **è®­ç»ƒæ ·æœ¬**: {metadata['dataset_info']['train_samples']}
- **éªŒè¯æ ·æœ¬**: {metadata['dataset_info']['val_samples']}
- **æµ‹è¯•æ ·æœ¬**: {metadata['dataset_info']['test_samples']}
- **æ€»æ ·æœ¬æ•°**: {sum(metadata['dataset_info'].values())}

## ç±»åˆ«æ˜ å°„
"""
    
    for class_name, index in metadata['class_indices'].items():
        summary += f"- {index}: {class_name}\n"
    
    summary += f"""
## è®­ç»ƒä¿¡æ¯
- **è®­ç»ƒæ—¶é•¿**: {metadata['training_time']:.2f} ç§’
- **è®­ç»ƒå®Œæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}

## æ–‡ä»¶è¾“å‡º
- **H5æ¨¡å‹**: food_classifier_{metadata['model_type']}_optimized.h5
- **TFLiteæ¨¡å‹**: food_classifier_{metadata['model_type']}_optimized.tflite
- **è®­ç»ƒå†å²å›¾**: training_history_{metadata['model_type']}_optimized.png
- **æ··æ·†çŸ©é˜µ**: confusion_matrix.png
- **è¯„ä¼°ç»“æœ**: evaluation_results.json
- **å…ƒæ•°æ®**: model_metadata.json

## ä½¿ç”¨å»ºè®®
1. H5æ¨¡å‹é€‚ç”¨äºæœåŠ¡å™¨ç«¯éƒ¨ç½²
2. TFLiteæ¨¡å‹é€‚ç”¨äºç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—
3. æŸ¥çœ‹è®­ç»ƒå†å²å›¾äº†è§£æ¨¡å‹æ”¶æ•›æƒ…å†µ
4. æŸ¥çœ‹æ··æ·†çŸ©é˜µåˆ†æç±»åˆ«é—´çš„æ··æ·†æƒ…å†µ
"""
    
    with open(output_dir / 'training_summary.md', 'w', encoding='utf-8') as f:
        f.write(summary)
    
    logger.info(f"è®­ç»ƒæ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {output_dir / 'training_summary.md'}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–çš„æ™ºèƒ½å†°ç®±AIæ¨¡å‹è®­ç»ƒè„šæœ¬")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True,
                       help="è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="ai_model/trained_models_optimized",
                       help="æ¨¡å‹è¾“å‡ºç›®å½•")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_type", type=str, default="mobilenet",
                       choices=["mobilenet", "resnet", "efficientnet"],
                       help="æ¨¡å‹æ¶æ„ç±»å‹")
    parser.add_argument("--image_size", type=int, default=224,
                       help="å›¾åƒè¾“å…¥å°ºå¯¸")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=30,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--fine_tune", action="store_true",
                       help="æ˜¯å¦è¿›è¡Œå¾®è°ƒ")
    parser.add_argument("--fine_tune_epochs", type=int, default=10,
                       help="å¾®è°ƒè½®æ•°")
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument("--plot_history", action="store_true", default=True,
                       help="æ˜¯å¦ç»˜åˆ¶è®­ç»ƒå†å²")
    parser.add_argument("--evaluate", action="store_true", default=True,
                       help="æ˜¯å¦è¯„ä¼°æ¨¡å‹")
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    log_file = Path(args.output_dir) / "training.log"
    setup_logging(getattr(logging, args.log_level), log_file)
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("=" * 60)
        logger.info("å¼€å§‹ä¼˜åŒ–çš„AIæ¨¡å‹è®­ç»ƒæµç¨‹")
        logger.info("=" * 60)
        logger.info(f"è®­ç»ƒå‚æ•°: {vars(args)}")
        
        # å¼€å§‹è®­ç»ƒ
        model_path, class_indices, metadata = train_optimized_model(args)
        
        # ç”Ÿæˆè®­ç»ƒæ€»ç»“
        generate_training_summary(Path(args.output_dir), metadata)
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ä¼˜åŒ–è®­ç»ƒæµç¨‹å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()